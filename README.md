# Sign-Language-Recognition-using-CNN

Our project aims to bridge the gap between the speech and hearing impaired people and the normal people. The basic idea of this project is to make a system using which disabled people can significantly communicate with all other people using their normal gestures. The project tries to recognise American Sign Language gestures.

We have used deep learning and convolutional neural networks to build a model that can recognize the American Sign Language alphabets with decent enough accuracy. For sign language recognition, convolutional neural network proved a strong candidature as a huge image dataset was used and CNNs are really effective for image classification as the concept of feature extraction and dimensionality reduction suits the huge number of parameters in an image.

Future scope of this proposed method can be it incorporating dynamic gestures and its deployment as a mobile application so that hearing and speech disabled community can benefit from it.

